# æ–‡æœ¬æ°´å°åŒç®—æ³•ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬ç³»ç»Ÿé›†æˆäº†ä¸¤ç§æ–‡æœ¬æ°´å°ç®—æ³•ï¼š
- **PostMark**ï¼ˆé»˜è®¤ï¼‰ï¼šåå¤„ç†æ°´å°ï¼Œæ”¯æŒé»‘ç›’LLM
- **CredID**ï¼šç”Ÿæˆæ—¶æ°´å°ï¼Œéœ€è¦è®¿é—®æ¨¡å‹logits


## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: ä½¿ç”¨é»˜è®¤PostMarkç®—æ³•

```python
from src.unified.watermark_tool import WatermarkTool

tool = WatermarkTool()

# PostMarkæ˜¯åå¤„ç†ï¼Œæ‰€ä»¥éœ€è¦å…ˆæœ‰ç”Ÿæˆçš„æ–‡æœ¬
generated_text = """
Your LLM generated text here...
This could be from GPT-4, Claude, or any other LLM.
"""

# åµŒå…¥æ°´å°
watermarked_text = tool.embed(generated_text, "my_watermark", 'text')

# æå–æ°´å°ï¼ˆéœ€è¦æä¾›åŸå§‹æ°´å°è¯åˆ—è¡¨ä»¥æé«˜å‡†ç¡®ç‡ï¼‰
result = tool.extract(watermarked_text, 'text',
                     original_words=["detected", "words", "list"])
print(f"æ£€æµ‹åˆ°æ°´å°: {result['detected']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.2%}")
```

### æ–¹å¼2: åˆ‡æ¢åˆ°CredIDç®—æ³•

#### ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config/text_config.yaml`:

```yaml
# å°†algorithmä»postmarkæ”¹ä¸ºcredid
algorithm: "credid"
```

#### ä½¿ç”¨CredID

```python
from src.unified.watermark_tool import WatermarkTool
from transformers import AutoModelForCausalLM, AutoTokenizer

tool = WatermarkTool()

# åŠ è½½æ¨¡å‹ï¼ˆCredIDéœ€è¦ï¼‰
model = AutoModelForCausalLM.from_pretrained("your-model")
tokenizer = AutoTokenizer.from_pretrained("your-model")

# CredIDæ˜¯ç”Ÿæˆæ—¶åµŒå…¥ï¼Œè¾“å…¥çš„æ˜¯prompt
prompt = "Tell me about AI:"

# åµŒå…¥æ°´å°ï¼ˆç”Ÿæˆæ—¶ï¼‰
watermarked_text = tool.embed(prompt, "my_watermark", 'text',
                              model=model, tokenizer=tokenizer)

# æå–æ°´å°
result = tool.extract(watermarked_text, 'text',
                     model=model, tokenizer=tokenizer,
                     candidates_messages=["my_watermark"])
print(f"æ£€æµ‹åˆ°æ°´å°: {result['detected']}")
print(f"æå–çš„æ¶ˆæ¯: {result['message']}")
```

## ğŸ“– è¯¦ç»†ç”¨æ³•

### PostMarkç®—æ³•è¯¦è§£

#### ä¾èµ–æ¨¡å‹

PostMarkéœ€è¦ä»¥ä¸‹æœ¬åœ°æ¨¡å‹ï¼ˆå·²ä¸‹è½½åˆ°æ‚¨çš„ç¯å¢ƒï¼‰:
- `nomic-ai/nomic-embed-text-v1` (åµŒå…¥æ¨¡å‹)
- `mistralai/Mistral-7B-Instruct-v0.2` (æ’å…¥LLM)
- `paragram_xxl.pkl` (ç›¸ä¼¼åº¦è®¡ç®—)
- `filtered_data_100k_unique_250w_sentbound_nomic_embs.pkl` (é¢„è®¡ç®—åµŒå…¥)

#### é…ç½®å‚æ•°

```yaml
postmark:
  embedder: "nomic"              # åµŒå…¥æ¨¡å‹
  inserter: "mistral-7b-inst"    # æ’å…¥LLM
  ratio: 0.12                    # æ°´å°è¯æ¯”ä¾‹ï¼ˆ12%ï¼‰
  iterate: "v2"                  # è¿­ä»£ç‰ˆæœ¬
  threshold: 0.7                 # æ£€æµ‹é˜ˆå€¼
```

#### å®Œæ•´ç¤ºä¾‹

```python
from src.text_watermark.postmark_watermark import PostMarkWatermark

# åˆå§‹åŒ–
config = {
    'embedder': 'nomic',
    'inserter': 'mistral-7b-inst',
    'ratio': 0.12,
    'iterate': 'v2',
    'threshold': 0.7
}
watermark = PostMarkWatermark(config)

# å·²ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆæ¥è‡ªä»»ä½•LLMï¼‰
text = "Your generated text here..."

# åµŒå…¥æ°´å°
result = watermark.embed(text, message="watermark_id")

# æå–æ°´å°ï¼ˆä½¿ç”¨åŸå§‹æ°´å°è¯åˆ—è¡¨ï¼‰
detection = watermark.extract(
    result['watermarked_text'],
    original_words=result['watermark_words']  # å…³é”®ï¼
)

print(f"æ°´å°å­˜åœ¨ç‡: {detection['presence_score']:.2%}")
```

### CredIDç®—æ³•è¯¦è§£

#### é…ç½®å‚æ•°

```yaml
credid:
  mode: "lm"                     # æ¨¡å¼ï¼šlmæˆ–random
  model_name: "huggyllama/llama-7b"
  lm_params:
    delta: 2.0                   # æ°´å°å¼ºåº¦
    message_len: 10              # æ¶ˆæ¯é•¿åº¦ï¼ˆä½ï¼‰
  wm_params:
    encode_ratio: 4              # ç¼–ç æ¯”ç‡
```

#### å®Œæ•´ç¤ºä¾‹

```python
from src.text_watermark.credid_watermark import CredIDWatermark
from transformers import AutoModelForCausalLM, AutoTokenizer

# åˆå§‹åŒ–
config = {
    'mode': 'lm',
    'model_name': 'gpt2',
    'lm_params': {'delta': 1.5, 'message_len': 10},
    'wm_params': {'encode_ratio': 8}
}
watermark = CredIDWatermark(config)

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('gpt2')
tokenizer = AutoTokenizer.from_pretrained('gpt2')

# åµŒå…¥ï¼ˆç”Ÿæˆæ—¶ï¼‰
result = watermark.embed(model, tokenizer,
                        prompt="AI is",
                        message="my_watermark")

# æå–
detection = watermark.extract(
    result['watermarked_text'],
    model=model,
    tokenizer=tokenizer,
    candidates_messages=["my_watermark", "wrong_msg"]
)

print(f"æå–çš„æ¶ˆæ¯: {detection['extracted_message']}")
```

## ğŸ”„ ç®—æ³•åˆ‡æ¢æ–¹æ³•

### æ–¹æ³•1: ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config/text_config.yaml`:

```yaml
# åˆ‡æ¢åˆ°PostMark
algorithm: "postmark"

# æˆ–åˆ‡æ¢åˆ°CredID
# algorithm: "credid"
```

### æ–¹æ³•2: ä»£ç åŠ¨æ€åˆ‡æ¢

```python
from src.text_watermark.text_watermark import TextWatermark

watermark = TextWatermark()

# æŸ¥çœ‹å½“å‰ç®—æ³•
print(f"å½“å‰ç®—æ³•: {watermark.get_algorithm()}")

# åˆ‡æ¢åˆ°CredID
watermark.set_algorithm('credid')

# åˆ‡æ¢åˆ°PostMark
watermark.set_algorithm('postmark')
```

### æ–¹æ³•3: UnifiedEngineåˆ‡æ¢

```python
from src.unified.unified_engine import UnifiedWatermarkEngine

engine = UnifiedWatermarkEngine()

# æŸ¥çœ‹é»˜è®¤ç®—æ³•
print(engine.get_default_algorithms())
# è¾“å‡º: {'text': 'postmark', 'image': 'videoseal', ...}

# å†…éƒ¨ä¼šè‡ªåŠ¨æ ¹æ®é…ç½®ä½¿ç”¨ç›¸åº”ç®—æ³•
```

## ğŸ§ª è¿è¡Œæµ‹è¯•

éªŒè¯åŒç®—æ³•ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```bash
# ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python tests/test_text_watermark_dual_algorithm.py
```

æµ‹è¯•å†…å®¹åŒ…æ‹¬ï¼š
1. PostMarkåŸºç¡€åŠŸèƒ½æµ‹è¯•
2. CredIDåŸºç¡€åŠŸèƒ½æµ‹è¯•
3. ç»Ÿä¸€æ¥å£ç®—æ³•åˆ‡æ¢æµ‹è¯•
4. é…ç½®æ–‡ä»¶ç®—æ³•åˆ‡æ¢æµ‹è¯•
5. UnifiedEngineé›†æˆæµ‹è¯•

## ğŸ“Œ é‡è¦æ³¨æ„äº‹é¡¹

### PostMarkç‰¹æ€§
1. **åå¤„ç†**ï¼šå¿…é¡»å¯¹å·²ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œå¤„ç†
2. **æ°´å°è¯åˆ—è¡¨**ï¼šæ£€æµ‹æ—¶æœ€å¥½æä¾›åŸå§‹æ°´å°è¯åˆ—è¡¨
3. **æœ¬åœ°æ¨¡å‹**ï¼šéœ€è¦Mistral-7Bç­‰å¤§æ¨¡å‹ï¼Œç¡®ä¿GPUå†…å­˜å……è¶³
4. **å¤„ç†æ—¶é—´**ï¼šåµŒå…¥è¿‡ç¨‹éœ€è¦1-5ç§’ï¼ˆå–å†³äºæ–‡æœ¬é•¿åº¦ï¼‰

### CredIDç‰¹æ€§
1. **ç”Ÿæˆæ—¶åµŒå…¥**ï¼šè¾“å…¥æ˜¯promptï¼Œè¾“å‡ºæ˜¯ç”Ÿæˆ+æ°´å°çš„æ–‡æœ¬
2. **æ¨¡å‹è¦æ±‚**ï¼šéœ€è¦å®Œæ•´çš„æ¨¡å‹å’Œåˆ†è¯å™¨è®¿é—®æƒé™
3. **å€™é€‰æ¶ˆæ¯**ï¼šæå–æ—¶æä¾›å€™é€‰æ¶ˆæ¯åˆ—è¡¨å¯æé«˜å‡†ç¡®ç‡
4. **å†…å­˜éœ€æ±‚**ï¼šéœ€è¦åŠ è½½å®Œæ•´LLMåˆ°å†…å­˜

### æ€§èƒ½å¯¹æ¯”
- **PostMark**ï¼šé€‚åˆç”Ÿäº§ç¯å¢ƒï¼Œæ”¯æŒä»»ä½•LLM
- **CredID**ï¼šé€‚åˆç ”ç©¶åœºæ™¯ï¼Œæ£€æµ‹ç‡ç•¥é«˜ä½†é™åˆ¶å¤š

## ğŸ”§ æ•…éšœæ’é™¤

### PostMarké—®é¢˜

**é—®é¢˜1: æ‰¾ä¸åˆ°PostMarkæ¨¡å—**
```bash
# æ£€æŸ¥PostMarkç›®å½•
ls src/text_watermark/PostMark/

# åº”è¯¥åŒ…å«ï¼špostmark/, paragram_xxl.pkl, ç­‰æ–‡ä»¶
```

**é—®é¢˜2: æ¨¡å‹æ–‡ä»¶ç¼ºå¤±**
```bash
# æ£€æŸ¥å…³é”®æ–‡ä»¶
ls src/text_watermark/PostMark/*.pkl

# åº”è¯¥æœ‰ï¼š
# - paragram_xxl.pkl
# - filtered_data_100k_unique_250w_sentbound_nomic_embs.pkl
# - valid_wtmk_words_in_wiki_base-only-f1000.pkl
```

**é—®é¢˜3: Mistralæ¨¡å‹æœªä¸‹è½½**
```bash
# æ£€æŸ¥HuggingFaceç¼“å­˜
ls ~/.cache/huggingface/models--mistralai--Mistral-7B-Instruct-v0.2/
```

### CredIDé—®é¢˜

**é—®é¢˜1: æ¨¡å‹åŠ è½½å¤±è´¥**
- ç¡®ä¿æ¨¡å‹åç§°æ­£ç¡®
- æ£€æŸ¥HuggingFaceç¼“å­˜ç›®å½•
- å°è¯•ä½¿ç”¨æ›´å°çš„æµ‹è¯•æ¨¡å‹ï¼ˆå¦‚`sshleifer/tiny-gpt2`ï¼‰

**é—®é¢˜2: å†…å­˜ä¸è¶³**
- å‡å°‘`max_new_tokens`
- ä½¿ç”¨CPUæ¨¡å¼ï¼ˆ`device: 'cpu'`ï¼‰
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹

## ğŸ“š ç›¸å…³æ–‡ä»¶

- é…ç½®æ–‡ä»¶: `config/text_config.yaml`
- PostMarkå°è£…: `src/text_watermark/postmark_watermark.py`
- CredIDå°è£…: `src/text_watermark/credid_watermark.py`
- ç»Ÿä¸€æ¥å£: `src/text_watermark/text_watermark.py`
- æµ‹è¯•è„šæœ¬: `tests/test_text_watermark_dual_algorithm.py`
- å¼•æ“é›†æˆ: `src/unified/unified_engine.py`

## ğŸ’¡ æœ€ä½³å®è·µ

### ä½¿ç”¨PostMarkçš„åœºæ™¯
- âœ… ä½¿ç”¨ç¬¬ä¸‰æ–¹APIï¼ˆOpenAIã€Anthropicç­‰ï¼‰
- âœ… æ— æ³•è®¿é—®æ¨¡å‹å†…éƒ¨
- âœ… éœ€è¦å¤„ç†å·²ç”Ÿæˆçš„æ–‡æœ¬
- âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### ä½¿ç”¨CredIDçš„åœºæ™¯
- âœ… è‡ªå·±éƒ¨ç½²çš„å¼€æºæ¨¡å‹
- âœ… å®Œå…¨æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹
- âœ… ç ”ç©¶å’Œå®éªŒ
- âœ… éœ€è¦æœ€é«˜æ£€æµ‹å‡†ç¡®ç‡

## ğŸ“ è¿›ä¸€æ­¥å­¦ä¹ 

- [PostMarkè®ºæ–‡](https://arxiv.org/abs/2403.07344)
- [CredIDæ–‡æ¡£](src/text_watermark/credid/README.md)
- [ç»Ÿä¸€æ¥å£æ–‡æ¡£](src/unified/README.md)
