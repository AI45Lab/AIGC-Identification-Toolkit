"""
Wan2.1æ–‡ç”Ÿè§†é¢‘ç”Ÿæˆå™¨
åŸºäºWan-AI/Wan2.1-T2V-1.3B-Diffusersæ¨¡å‹çš„æ–‡æœ¬åˆ°è§†é¢‘ç”ŸæˆåŠŸèƒ½
"""

import os
import logging
import torch
import numpy as np
from typing import Optional, Union, Dict, Any
from pathlib import Path

from .model_manager import ModelManager

# å°è¯•å¯¼å…¥diffusersç›¸å…³æ¨¡å—
try:
    from diffusers import WanPipeline, AutoencoderKLWan
    from diffusers.utils import export_to_video
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False
    logging.warning("diffusers not available. Please install with: pip install diffusers")


class WanVideoGenerator:
    """Wan2.1æ–‡ç”Ÿè§†é¢‘ç”Ÿæˆå™¨"""

    def __init__(self, model_manager: ModelManager, device: Optional[str] = None):
        """
        åˆå§‹åŒ–Wan2.1ç”Ÿæˆå™¨

        Args:
            model_manager: æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
            device: è®¡ç®—è®¾å¤‡ ('cuda', 'cpu', æˆ–Noneè‡ªåŠ¨é€‰æ‹©)
        """
        self.model_manager = model_manager
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.pipeline = None

        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)

        # æ£€æŸ¥ä¾èµ–
        if not DIFFUSERS_AVAILABLE:
            raise ImportError(
                "diffusers is required for Wan2.1 generation. "
                "Install with: pip install diffusers torch torchvision"
            )

    def _load_pipeline(self, allow_download: bool = False):
        """
        å»¶è¿ŸåŠ è½½Wan2.1ç®¡é“

        å…³é”®ç‰¹æ€§ï¼š
        - ä½¿ç”¨trust_remote_code=Trueï¼ˆå¿…éœ€ï¼‰
        - VAEä½¿ç”¨float32æ•°æ®ç±»å‹
        - Pipelineä½¿ç”¨bfloat16ï¼ˆCUDAï¼‰æˆ–float32ï¼ˆCPUï¼‰
        - ä»…åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆlocal_files_only=Trueï¼‰
        """
        if self.pipeline is not None:
            return

        self.logger.info("æ­£åœ¨åŠ è½½Wan2.1ç®¡é“...")

        try:
            # ä»…ä½¿ç”¨æœ¬åœ°å¿«ç…§è·¯å¾„
            try:
                local_model_path = self.model_manager.ensure_wan_model(allow_download=allow_download)
            except Exception as e:
                raise RuntimeError(
                    f"æœªæ‰¾åˆ°æœ¬åœ°Wan2.1æ¨¡å‹ï¼Œè¯·ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½åˆ°: "
                    f"{self.model_manager.cache_dir}/models--Wan-AI--Wan2.1-T2V-1.3B-Diffusers\n"
                    f"é”™è¯¯è¯¦æƒ…: {e}"
                )

            self.logger.info(f"ä»æœ¬åœ°å¿«ç…§åŠ è½½Wan2.1: {local_model_path}")

            # æ­¥éª¤1: åŠ è½½VAEï¼ˆå¿…é¡»ä½¿ç”¨float32ï¼Œè¿™æ˜¯Wan2.1çš„è¦æ±‚ï¼‰
            self.logger.info("åŠ è½½AutoencoderKLWan (VAE)...")
            vae = AutoencoderKLWan.from_pretrained(
                local_model_path,
                subfolder="vae",
                torch_dtype=torch.float32,  # VAEå¿…é¡»ä½¿ç”¨float32
                local_files_only=True,
                trust_remote_code=True  # Wan2.1å¿…éœ€å‚æ•°
            )
            self.logger.info("âœ“ VAEåŠ è½½å®Œæˆ (torch.float32)")

            # æ­¥éª¤2: åŠ è½½Pipeline
            # CUDAä½¿ç”¨bfloat16ä»¥èŠ‚çœæ˜¾å­˜ï¼ŒCPUä½¿ç”¨float32
            torch_dtype = torch.bfloat16 if self.device == 'cuda' else torch.float32
            self.logger.info(f"åŠ è½½WanPipeline (torch_dtype={torch_dtype})...")

            self.pipeline = WanPipeline.from_pretrained(
                local_model_path,
                vae=vae,  # ä¼ å…¥å·²åŠ è½½çš„VAE
                torch_dtype=torch_dtype,
                local_files_only=True,
                trust_remote_code=True  # Wan2.1å¿…éœ€å‚æ•°
            )
            self.logger.info("âœ“ PipelineåŠ è½½å®Œæˆ")

            # æ­¥éª¤3: ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            self.pipeline.to(self.device)
            self.logger.info(f"âœ“ Pipelineå·²ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")

            # æ­¥éª¤4: å†…å­˜ä¼˜åŒ–ï¼ˆå¤ç”¨HunyuanVideoçš„ä¼˜åŒ–ç­–ç•¥ï¼‰
            if hasattr(self.pipeline, 'vae') and hasattr(self.pipeline.vae, 'enable_tiling'):
                self.pipeline.vae.enable_tiling()
                self.logger.info("âœ“ å¯ç”¨VAE tilingï¼ˆé™ä½æ˜¾å­˜å ç”¨ï¼‰")

            # CPU offloadä¼˜åŒ–ï¼ˆä»…CUDAï¼‰
            if self.device == 'cuda' and hasattr(self.pipeline, 'enable_model_cpu_offload'):
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰device_mapï¼Œé¿å…å†²çª
                using_device_map = hasattr(self.pipeline, 'hf_device_map') and self.pipeline.hf_device_map is not None
                if not using_device_map:
                    self.pipeline.enable_model_cpu_offload()
                    self.logger.info("âœ“ å¯ç”¨æ¨¡å‹CPU offload")
                else:
                    self.logger.info("âš  æ£€æµ‹åˆ°device_mapï¼Œè·³è¿‡CPU offloadä»¥é¿å…å†²çª")

            self.logger.info(f"ğŸ‰ Wan2.1ç®¡é“åŠ è½½å®Œæˆï¼è®¾å¤‡: {self.device}, æ•°æ®ç±»å‹: {torch_dtype}")
            self.logger.info("âš  å®‰å…¨æç¤º: trust_remote_code=True ä»…åœ¨æœ¬åœ°æ¨¡å¼ä¸‹ä½¿ç”¨")

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½Wan2.1ç®¡é“å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            raise RuntimeError(f"Failed to load Wan2.1 pipeline: {e}")

    def generate_video(
        self,
        prompt: str,
        negative_prompt: Optional[str] = None,
        num_frames: int = 81,  # Wan2.1æ¨èï¼š81å¸§ï¼ˆ5ç§’@15fpsï¼‰
        height: int = 480,      # Wan2.1æ¨èï¼š480p
        width: int = 832,       # Wan2.1æ¨èï¼š832ï¼ˆ16:9æ¯”ä¾‹ï¼‰
        num_inference_steps: int = 50,  # Wan2.1æ¨èï¼š50æ­¥
        guidance_scale: float = 5.0,    # Wan2.1æ¨èï¼š5.0
        seed: Optional[int] = None,
        output_path: Optional[str] = None
    ) -> Union[torch.Tensor, str]:
        """
        ç”Ÿæˆè§†é¢‘

        Args:
            prompt: æ–‡æœ¬æç¤ºè¯
            negative_prompt: è´Ÿå‘æç¤ºè¯ï¼ˆæ¨èä½¿ç”¨ä»¥æå‡è´¨é‡ï¼‰
            num_frames: è§†é¢‘å¸§æ•° (æ¨è81å¸§=5ç§’@15fpsï¼ŒWan2.1æ— ä¸¥æ ¼æ ¼å¼é™åˆ¶)
            height: è§†é¢‘é«˜åº¦ (æ¨è480pï¼Œæ”¯æŒ720pä½†ä¸ç¨³å®š)
            width: è§†é¢‘å®½åº¦ (æ¨è832ï¼Œ16:9æ¯”ä¾‹)
            num_inference_steps: æ¨ç†æ­¥æ•° (æ¨è50ï¼Œè¶Šé«˜è´¨é‡è¶Šå¥½ä½†è¶Šæ…¢)
            guidance_scale: å¼•å¯¼å¼ºåº¦ (æ¨è5.0)
            seed: éšæœºç§å­
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼Œå¦‚æœNoneåˆ™è¿”å›tensor

        Returns:
            torch.Tensor or str: è§†é¢‘tensoræˆ–è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self._load_pipeline(allow_download=False)  # ä»…ä½¿ç”¨æœ¬åœ°æ¨¡å‹

        self.logger.info(f"å¼€å§‹ç”Ÿæˆè§†é¢‘: '{prompt[:50]}...'")
        self.logger.info(f"å‚æ•°: {num_frames}å¸§, {height}x{width}, {num_inference_steps}æ­¥, CFG={guidance_scale}")

        # è®¾ç½®æ¨èçš„è´Ÿå‘æç¤ºè¯ï¼ˆå¦‚æœç”¨æˆ·æœªæä¾›ï¼‰
        if negative_prompt is None:
            negative_prompt = (
                "Bright tones, overexposed, static, blurred details, "
                "unclear limbs, worst quality, low quality"
            )
            self.logger.info(f"ä½¿ç”¨é»˜è®¤è´Ÿå‘æç¤ºè¯: {negative_prompt[:50]}...")

        # è®¾ç½®éšæœºç§å­
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
            self.logger.info(f"è®¾ç½®éšæœºç§å­: {seed}")

        try:
            # ç”Ÿæˆè§†é¢‘ï¼ˆå¸¦OOMè‡ªé€‚åº”é‡è¯•ï¼Œå¤ç”¨HunyuanVideoçš„é€»è¾‘ï¼‰
            attempt = 0
            max_attempts = 3
            current_params = {
                'num_frames': num_frames,
                'height': height,
                'width': width,
                'num_inference_steps': num_inference_steps
            }

            while attempt < max_attempts:
                try:
                    with torch.no_grad():
                        result = self.pipeline(
                            prompt=prompt,
                            negative_prompt=negative_prompt,
                            num_frames=current_params['num_frames'],
                            height=current_params['height'],
                            width=current_params['width'],
                            num_inference_steps=current_params['num_inference_steps'],
                            guidance_scale=guidance_scale,
                            generator=generator
                        )
                    break  # ç”ŸæˆæˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯

                except RuntimeError as re:
                    # æ•è·CUDA OOMå¹¶è‡ªé€‚åº”é™ä½å‚æ•°é‡è¯•
                    message = str(re)
                    if ('CUDA out of memory' in message or 'out of memory' in message) and attempt < max_attempts - 1:
                        self.logger.warning(f"âš  æ£€æµ‹åˆ°CUDA OOMï¼Œè¿›è¡Œè‡ªé€‚åº”é‡è¯• (attempt {attempt + 1}/{max_attempts})")
                        self.logger.warning(f"OOMè¯¦æƒ…: {message[:200]}...")

                        attempt += 1

                        # æ¸…ç†GPUç¼“å­˜
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()

                        # é™ä½å‚æ•°ç­–ç•¥
                        current_params['height'] = max(256, current_params['height'] // 2)
                        current_params['width'] = max(256, current_params['width'] // 2)
                        current_params['num_frames'] = max(25, current_params['num_frames'] // 2)
                        current_params['num_inference_steps'] = max(20, current_params['num_inference_steps'] - 10)

                        self.logger.info(
                            f"é‡è¯•å‚æ•° -> frames: {current_params['num_frames']}, "
                            f"size: {current_params['height']}x{current_params['width']}, "
                            f"steps: {current_params['num_inference_steps']}"
                        )
                        continue
                    raise  # å…¶ä»–é”™è¯¯æˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç›´æ¥æŠ›å‡º

            # æå–è§†é¢‘å¸§ï¼ˆWan2.1æ ‡å‡†è¾“å‡ºæ ¼å¼: result.frames[0]ï¼‰
            self.logger.info(f"ç®¡é“è¾“å‡ºç±»å‹: {type(result)}")

            if hasattr(result, 'frames') and result.frames is not None:
                video_frames = result.frames[0]
                self.logger.info(f"âœ“ ä»result.frames[0]æå–è§†é¢‘å¸§")
            elif isinstance(result, (list, tuple)) and len(result) > 0:
                video_frames = result[0]
                self.logger.info(f"âœ“ ä»result[0]æå–è§†é¢‘å¸§")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {type(result)}")

            # éªŒè¯è¾“å‡ºç±»å‹
            if isinstance(video_frames, list) and len(video_frames) > 0:
                self.logger.info(f"âœ“ è§†é¢‘å¸§ç±»å‹: list, é•¿åº¦: {len(video_frames)}")
                if hasattr(video_frames[0], 'size'):
                    self.logger.info(f"âœ“ ç¬¬ä¸€å¸§å°ºå¯¸: {video_frames[0].size}")
            else:
                self.logger.warning(f"âš  æ„å¤–çš„è§†é¢‘å¸§ç±»å‹: {type(video_frames)}")

            self.logger.info(f"ğŸ¬ è§†é¢‘ç”Ÿæˆå®Œæˆï¼")

            # å¦‚æœæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œä¿å­˜è§†é¢‘æ–‡ä»¶
            if output_path:
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)

                # ä½¿ç”¨diffusersçš„export_to_videoä¿å­˜ (å®˜æ–¹æ¨èfps=15)
                export_to_video(video_frames, output_path, fps=15)

                self.logger.info(f"âœ“ è§†é¢‘å·²ä¿å­˜åˆ°: {output_path}")
                return output_path
            else:
                # è¿”å›video_framesï¼ˆPIL.Imageåˆ—è¡¨ï¼‰
                return video_frames

        except Exception as e:
            # CPUå›é€€ç­–ç•¥ï¼ˆå¤ç”¨HunyuanVideoçš„é€»è¾‘ï¼‰
            message = str(e)
            if self.device != 'cpu' and ('CUDA out of memory' in message or 'out of memory' in message):
                try:
                    self.logger.warning("âš  æŒç»­OOMï¼Œå°è¯•åˆ‡æ¢åˆ°CPUå¹¶ä»¥æ›´å°å‚æ•°é‡è¯•")

                    # åˆ‡æ¢åˆ°CPU
                    self.pipeline = self.pipeline.to('cpu')
                    self.device = 'cpu'

                    # è¿›ä¸€æ­¥é™ä½å‚æ•°
                    retry_frames = max(25, num_frames // 2)
                    retry_height = max(256, height // 2)
                    retry_width = max(256, width // 2)
                    retry_steps = max(20, num_inference_steps - 15)

                    with torch.no_grad():
                        result = self.pipeline(
                            prompt=prompt,
                            negative_prompt=negative_prompt,
                            num_frames=retry_frames,
                            height=retry_height,
                            width=retry_width,
                            num_inference_steps=retry_steps,
                            guidance_scale=guidance_scale,
                            generator=torch.Generator(device='cpu').manual_seed(seed) if seed else None
                        )

                    # æå–è¾“å‡º
                    if hasattr(result, 'frames') and result.frames is not None:
                        video_frames = result.frames[0]
                    elif isinstance(result, (list, tuple)) and len(result) > 0:
                        video_frames = result[0]
                    else:
                        video_frames = result

                    self.logger.info(f"âœ“ CPUå›é€€æˆåŠŸ: frames={retry_frames}, size={retry_height}x{retry_width}, steps={retry_steps}")

                    if output_path:
                        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
                        export_to_video(video_frames, output_path, fps=15)
                        self.logger.info(f"âœ“ è§†é¢‘å·²ä¿å­˜åˆ°: {output_path}")
                        return output_path
                    return video_frames

                except Exception as e_cpu:
                    self.logger.error(f"âŒ CPUå›é€€ä»å¤±è´¥: {e_cpu}")
                    # ç»§ç»­æŠ›å‡ºåŸå§‹é”™è¯¯

            self.logger.error(f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            raise RuntimeError(f"Failed to generate video: {e}")

    def generate_video_tensor(
        self,
        prompt: str,
        **kwargs
    ) -> torch.Tensor:
        """
        ç”Ÿæˆè§†é¢‘tensor (ç”¨äºåç»­æ°´å°å¤„ç†)

        Args:
            prompt: æ–‡æœ¬æç¤ºè¯
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°

        Returns:
            torch.Tensor: è§†é¢‘tensorï¼Œå½¢çŠ¶ä¸º (frames, channels, height, width)ï¼Œå€¼åŸŸ[0,1]
        """
        # å¼ºåˆ¶ä¸ä¿å­˜æ–‡ä»¶ï¼Œåªè¿”å›tensor
        kwargs['output_path'] = None
        video_frames = self.generate_video(prompt, **kwargs)

        # è½¬æ¢ä¸ºtorch tensor
        self.logger.info(f"generate_video_tensor æ”¶åˆ°æ•°æ®ç±»å‹: {type(video_frames)}")

        # Wan2.1è¾“å‡ºæ ¼å¼ï¼šPIL.Imageåˆ—è¡¨
        if isinstance(video_frames, list) and video_frames and hasattr(video_frames[0], 'convert'):
            from PIL import Image
            self.logger.info(f"âœ“ æ£€æµ‹åˆ°PILå›¾åƒåˆ—è¡¨ï¼Œé•¿åº¦: {len(video_frames)}")

            # è½¬æ¢PILå›¾åƒä¸ºnumpyæ•°ç»„
            frames = []
            for i, img in enumerate(video_frames):
                if isinstance(img, Image.Image):
                    # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
                    img_rgb = img.convert('RGB')
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„ (H, W, C)
                    frame_array = np.array(img_rgb)
                    frames.append(frame_array)

                    # è®°å½•ç¬¬ä¸€å¸§ä¿¡æ¯
                    if i == 0:
                        self.logger.info(f"  ç¬¬ä¸€å¸§å°ºå¯¸: {img.size}, æ•°ç»„shape: {frame_array.shape}, å€¼åŸŸ: [{frame_array.min()}, {frame_array.max()}]")

            # å †å æ‰€æœ‰å¸§ (frames, height, width, channels)
            video_array = np.stack(frames, axis=0)
            self.logger.info(f"  å †å åæ•°ç»„shape: {video_array.shape}")

            # è½¬æ¢ä¸ºtensorå¹¶è°ƒæ•´ç»´åº¦ (frames, channels, height, width)
            video_tensor = torch.from_numpy(video_array).permute(0, 3, 1, 2).float()

            # å½’ä¸€åŒ–åˆ°[0, 1]
            video_tensor = video_tensor / 255.0
            self.logger.info(f"âœ“ è½¬æ¢å®Œæˆ: tensor shape={video_tensor.shape}, å€¼åŸŸ=[{video_tensor.min():.3f}, {video_tensor.max():.3f}]")

        elif isinstance(video_frames, np.ndarray):
            # ç›´æ¥æ˜¯numpyæ•°ç»„ï¼ˆä¸å¤ªå¯èƒ½ï¼Œä½†ä¿ç•™å…¼å®¹æ€§ï¼‰
            self.logger.info(f"æ”¶åˆ°numpyæ•°ç»„: shape={video_frames.shape}")
            video_tensor = torch.from_numpy(video_frames).permute(0, 3, 1, 2).float()
            if video_tensor.max() > 1.0:
                video_tensor = video_tensor / 255.0

        elif torch.is_tensor(video_frames):
            # ç›´æ¥æ˜¯tensorï¼ˆä¸å¤ªå¯èƒ½ï¼Œä½†ä¿ç•™å…¼å®¹æ€§ï¼‰
            self.logger.info(f"æ”¶åˆ°torch tensor: shape={video_frames.shape}")
            video_tensor = video_frames.float()
            if video_tensor.max() > 1.0:
                video_tensor = video_tensor / 255.0
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„video_framesç±»å‹: {type(video_frames)}")

        self.logger.info(f"ğŸ¬ æœ€ç»ˆtensorå½¢çŠ¶: {video_tensor.shape}, å€¼åŸŸ: [{video_tensor.min():.3f}, {video_tensor.max():.3f}]")
        return video_tensor

    def get_pipeline_info(self) -> Dict[str, Any]:
        """è·å–ç®¡é“ä¿¡æ¯"""
        info = {
            "model": "Wan2.1-T2V-1.3B-Diffusers",
            "device": self.device,
            "pipeline_loaded": self.pipeline is not None,
            "diffusers_available": DIFFUSERS_AVAILABLE,
            "trust_remote_code": True,  # æ ‡è®°ä½¿ç”¨äº†trust_remote_code
            "recommended_params": {
                "num_frames": 81,
                "height": 480,
                "width": 832,
                "num_inference_steps": 50,
                "guidance_scale": 5.0
            }
        }

        if self.pipeline is not None:
            info.update({
                "dtype": str(self.pipeline.dtype) if hasattr(self.pipeline, 'dtype') else 'unknown',
                "components": list(self.pipeline.components.keys()) if hasattr(self.pipeline, 'components') else []
            })

        return info

    def clear_pipeline(self):
        """æ¸…ç†ç®¡é“ä»¥é‡Šæ”¾å†…å­˜"""
        if self.pipeline is not None:
            del self.pipeline
            self.pipeline = None

            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            self.logger.info("Wan2.1ç®¡é“å·²æ¸…ç†")


# æ–¹ä¾¿çš„å·¥å…·å‡½æ•°
def create_wan_generator(
    cache_dir: Optional[str] = None,
    device: Optional[str] = None
) -> WanVideoGenerator:
    """
    åˆ›å»ºWan2.1ç”Ÿæˆå™¨çš„å¿«æ·å‡½æ•°

    Args:
        cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        device: è®¡ç®—è®¾å¤‡

    Returns:
        WanVideoGenerator: ç”Ÿæˆå™¨å®ä¾‹
    """
    model_manager = ModelManager(cache_dir) if cache_dir else ModelManager()
    return WanVideoGenerator(model_manager, device)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import sys

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("=" * 60)
    print("Wan2.1 Video Generator æµ‹è¯•")
    print("=" * 60)

    try:
        generator = create_wan_generator()

        # æ˜¾ç¤ºç”Ÿæˆå™¨ä¿¡æ¯
        info = generator.get_pipeline_info()
        print("\nç”Ÿæˆå™¨ä¿¡æ¯:")
        for key, value in info.items():
            print(f"  {key}: {value}")

        # å¦‚æœå‘½ä»¤è¡Œå‚æ•°åŒ…å«testï¼Œè¿›è¡Œå®é™…ç”Ÿæˆæµ‹è¯•
        if len(sys.argv) > 1 and sys.argv[1] == "test":
            print("\n" + "=" * 60)
            print("å¼€å§‹ç”Ÿæˆæµ‹è¯•è§†é¢‘...")
            print("=" * 60)

            # ç”Ÿæˆä¸€ä¸ªçŸ­è§†é¢‘ç”¨äºæµ‹è¯•
            test_prompt = "A cat walks on the grass, realistic"

            video_tensor = generator.generate_video_tensor(
                prompt=test_prompt,
                num_frames=81,   # Wan2.1æ¨è
                height=480,      # Wan2.1æ¨è
                width=832,       # Wan2.1æ¨è
                num_inference_steps=50,
                guidance_scale=5.0,
                seed=42
            )

            print(f"\nâœ… æµ‹è¯•è§†é¢‘ç”Ÿæˆå®Œæˆ!")
            print(f"   Tensor shape: {video_tensor.shape}")
            print(f"   å€¼åŸŸ: [{video_tensor.min():.3f}, {video_tensor.max():.3f}]")

            # ä¹Ÿå¯ä»¥ä¿å­˜ä¸ºæ–‡ä»¶
            output_path = "test_wan_output.mp4"
            generator.generate_video(
                prompt=test_prompt,
                num_frames=81,
                height=480,
                width=832,
                num_inference_steps=50,
                guidance_scale=5.0,
                seed=42,
                output_path=output_path
            )

            print(f"\nâœ… æµ‹è¯•è§†é¢‘å·²ä¿å­˜: {output_path}")
            print("=" * 60)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
