#!/usr/bin/env python3
"""
Image-Bench å›¾åƒæ°´å°é²æ£’æ€§è¯„ä¼°è„šæœ¬

ç”¨æ³•:
    # ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆVideoSealï¼‰
    python image_benchmark.py

    # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
    python image_benchmark.py --config benchmarks/Image-Bench/configs/my_algorithm.yaml

    # å¿«é€Ÿæµ‹è¯•ï¼ˆä»…10å¼ å›¾åƒï¼‰
    python image_benchmark.py --max-images 10

    # æŒ‡å®šGPUè®¾å¤‡
    python image_benchmark.py --device cuda:1
"""

import sys
import argparse
import importlib.util
from pathlib import Path


def load_benchmark_runner():
    """åŠ¨æ€åŠ è½½ BenchmarkRunnerï¼ˆå¤„ç†å¸¦è¿å­—ç¬¦çš„ç›®å½•åï¼‰"""
    spec = importlib.util.spec_from_file_location(
        'benchmark_runner',
        'benchmarks/Image-Bench/evaluators/benchmark_runner.py'
    )
    runner_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(runner_module)
    return runner_module.BenchmarkRunner


def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‚æ•°å¹¶è¿è¡Œè¯„ä¼°"""
    parser = argparse.ArgumentParser(
        description='Image-Bench å›¾åƒæ°´å°é²æ£’æ€§è¯„ä¼°',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # VideoSeal å®Œæ•´è¯„ä¼°ï¼ˆ1000å¼ å›¾åƒï¼‰
  python image_benchmark.py

  # PRC-Watermark è¯„ä¼°
  python image_benchmark.py --config benchmarks/Image-Bench/configs/prc_distortion.yaml

  # å¿«é€Ÿæµ‹è¯•ï¼ˆ10å¼ å›¾åƒï¼‰
  python image_benchmark.py --max-images 10

  # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  python image_benchmark.py --output benchmarks/Image-Bench/results/my_test
        """
    )

    parser.add_argument(
        '--config', '-c',
        type=str,
        default='benchmarks/Image-Bench/configs/videoseal_distortion.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: VideoSealé…ç½®ï¼‰'
    )

    parser.add_argument(
        '--max-images', '-n',
        type=int,
        default=None,
        help='æœ€å¤§æµ‹è¯•å›¾åƒæ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼Œé»˜è®¤: ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰'
    )

    parser.add_argument(
        '--device', '-d',
        type=str,
        default=None,
        help='è®¡ç®—è®¾å¤‡ï¼ˆcuda/cpuï¼Œé»˜è®¤: ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰'
    )

    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®ï¼‰'
    )

    args = parser.parse_args()

    # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
    sys.path.insert(0, '.')

    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print(f"   å¯ç”¨é…ç½®: benchmarks/Image-Bench/configs/")
        return 1

    print("=" * 70)
    print("ğŸš€ Image-Bench å›¾åƒæ°´å°é²æ£’æ€§è¯„ä¼°")
    print("=" * 70)
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {args.config}")
    if args.max_images:
        print(f"ğŸ–¼ï¸  æµ‹è¯•å›¾åƒ: {args.max_images} å¼ ï¼ˆå¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼‰")
    if args.device:
        print(f"ğŸ’» è®¡ç®—è®¾å¤‡: {args.device}")
    if args.output:
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
    print("=" * 70)
    print()

    try:
        # åŠ è½½ BenchmarkRunner
        BenchmarkRunner = load_benchmark_runner()

        # åˆ›å»ºè¿è¡Œå™¨
        runner = BenchmarkRunner(str(config_path))

        # TODO: æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
        # è¿™éœ€è¦ä¿®æ”¹ BenchmarkRunner çš„å®ç°æ¥æ”¯æŒè¿è¡Œæ—¶é…ç½®è¦†ç›–

        # è¿è¡Œè¯„ä¼°
        results = runner.run(max_images=args.max_images)

        print("\n" + "=" * 70)
        print("ğŸ‰ è¯„ä¼°å®Œæˆ!")
        print("=" * 70)
        print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {runner.output_dir / 'metrics.json'}")
        print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - æ°´å°å›¾åƒ: {runner.output_dir / 'watermarked/'}")
        print(f"   - æ”»å‡»å›¾åƒ: {runner.output_dir / 'attacked/'}")
        print(f"   - è¯„ä¼°æŒ‡æ ‡: {runner.output_dir / 'metrics.json'}")
        print()

        # æ˜¾ç¤ºç®€è¦ç»“æœæ‘˜è¦
        if results and 'quality_metrics' in results:
            print("ğŸ“ˆ è´¨é‡æŒ‡æ ‡æ‘˜è¦ï¼ˆåŸå›¾ vs æ°´å°å›¾ï¼‰:")
            for metric, value in results['quality_metrics'].items():
                print(f"   {metric.upper()}: {value:.4f}")

        if results and 'robustness_by_attack' in results:
            print("\nğŸ¯ é²æ£’æ€§æ‘˜è¦:")
            for attack_type, strengths in results['robustness_by_attack'].items():
                if strengths:
                    rates = [s['detection_rate'] for s in strengths.values()]
                    avg_rate = sum(rates) / len(rates)
                    print(f"   {attack_type:<12}: {avg_rate:.2%} å¹³å‡æ£€æµ‹ç‡")

        print()
        return 0

    except KeyboardInterrupt:
        print("\n\nâš ï¸  è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except FileNotFoundError as e:
        print(f"\n\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        print("   è¯·ç¡®è®¤å·²ä¸‹è½½ Image-Bench DISTORTION_1K æ•°æ®é›†ï¼Œå¹¶å°† PNG å›¾åƒæ”¾åœ¨é…ç½®æŒ‡å®šçš„ç›®å½•ä¸‹ã€‚")
        return 1
    except Exception as e:
        print(f"\n\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())