#!/usr/bin/env python3
"""
PostMark HuggingFaceæ¨¡å‹ä¸‹è½½è„šæœ¬
è‡ªåŠ¨ä¸‹è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹ï¼ˆåˆ©ç”¨HuggingFaceé»˜è®¤ç¼“å­˜æœºåˆ¶ï¼‰
æ”¯æŒä½¿ç”¨å›½å†…é•œåƒç«™ç‚¹åŠ é€Ÿä¸‹è½½
"""

import os
import sys
from pathlib import Path

# è®¾ç½®HuggingFaceé•œåƒï¼ˆå›½å†…åŠ é€Ÿï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
print("âœ… å·²è®¾ç½®HuggingFaceé•œåƒ: https://hf-mirror.com")

# HuggingFaceé»˜è®¤ç¼“å­˜ç›®å½•ï¼ˆæ— éœ€æ˜¾å¼æŒ‡å®šï¼Œè‡ªåŠ¨ä½¿ç”¨ï¼‰
CACHE_DIR = Path.home() / ".cache/huggingface/hub"
print(f"âœ… æ¨¡å‹å°†è‡ªåŠ¨ä¸‹è½½åˆ°HuggingFaceé»˜è®¤ç¼“å­˜ç›®å½•")
print(f"   é¢„æœŸè·¯å¾„: {CACHE_DIR}")

print("=" * 60)
print("PostMark HuggingFaceæ¨¡å‹ä¸‹è½½è„šæœ¬")
print("=" * 60)
print(f"ç¼“å­˜ç›®å½•: {CACHE_DIR}")
print("=" * 60)

def check_gpu():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            return True
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆä¸‹è½½é€Ÿåº¦è¾ƒæ…¢ï¼‰")
            return False
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install torch")
        sys.exit(1)

def download_bert_tokenizer():
    """ä¸‹è½½BERT tokenizerï¼ˆNomicä¾èµ–ï¼‰"""
    print("\n[1/3] ä¸‹è½½ BERT Tokenizer...")
    print("-" * 60)
    print("è¯´æ˜: NomicEmbedéœ€è¦bert-base-uncased tokenizer")
    try:
        from transformers import AutoTokenizer
        # ä¸æŒ‡å®šcache_dirï¼Œä½¿ç”¨HuggingFaceé»˜è®¤è·¯å¾„
        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        print("âœ… bert-base-uncased tokenizer ä¸‹è½½æˆåŠŸ")
        model_path = CACHE_DIR / "models--bert-base-uncased"
        if model_path.exists():
            print(f"   å­˜å‚¨è·¯å¾„: {model_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def download_nomic_embedder():
    """ä¸‹è½½Nomic embeddingæ¨¡å‹"""
    print("\n[2/3] ä¸‹è½½ Nomic Embedding æ¨¡å‹...")
    print("-" * 60)
    print("æ¨¡å‹: nomic-ai/nomic-embed-text-v1")
    print("å¤§å°: ~550MB")
    print("ç”¨é€”: ç”Ÿæˆæ°´å°è¯åµŒå…¥")

    try:
        from transformers import AutoModel
        print("å¼€å§‹ä¸‹è½½... (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)")
        # ä¸æŒ‡å®šcache_dirï¼Œä½¿ç”¨HuggingFaceé»˜è®¤è·¯å¾„
        model = AutoModel.from_pretrained(
            "nomic-ai/nomic-embed-text-v1",
            trust_remote_code=True
        )
        print("âœ… nomic-ai/nomic-embed-text-v1 ä¸‹è½½æˆåŠŸ")
        model_path = CACHE_DIR / "models--nomic-ai--nomic-embed-text-v1"
        if model_path.exists():
            print(f"   å­˜å‚¨è·¯å¾„: {model_path}")

        # é‡Šæ”¾å†…å­˜
        del model
        import gc
        gc.collect()
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def download_llama3_inserter():
    """ä¸‹è½½Llama-3-8B-Instructæ¨¡å‹"""
    print("\n[3/3] ä¸‹è½½ Llama-3-8B-Instruct æ¨¡å‹...")
    print("-" * 60)
    print("æ¨¡å‹: meta-llama/Meta-Llama-3-8B-Instruct")
    print("å¤§å°: ~16GB")
    print("ç”¨é€”: æ°´å°è¯æ’å…¥")
    print("âš ï¸  æ³¨æ„: éœ€è¦åœ¨HuggingFaceæ¥å—Llama 3ä½¿ç”¨åè®®")
    print("   è®¿é—®: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct")

    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer

        model_name = "meta-llama/Meta-Llama-3-8B-Instruct"

        print("\nä¸‹è½½ tokenizer...")
        # ä¸æŒ‡å®šcache_dirï¼Œä½¿ç”¨HuggingFaceé»˜è®¤è·¯å¾„
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("âœ… Tokenizer ä¸‹è½½æˆåŠŸ")

        print("\nä¸‹è½½ æ¨¡å‹... (è¿™å¯èƒ½éœ€è¦10-30åˆ†é’Ÿï¼Œå–å†³äºç½‘é€Ÿ)")
        print("æç¤º: å¦‚æœä¸‹è½½ä¸­æ–­ï¼Œé‡æ–°è¿è¡Œæ­¤è„šæœ¬ä¼šè‡ªåŠ¨ç»­ä¼ ")

        # ä¸æŒ‡å®šcache_dirï¼Œä½¿ç”¨HuggingFaceé»˜è®¤è·¯å¾„
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map=None,  # ä¸åŠ è½½åˆ°GPUï¼Œä»…ä¸‹è½½
            low_cpu_mem_usage=True
        )
        print("âœ… meta-llama/Meta-Llama-3-8B-Instruct ä¸‹è½½æˆåŠŸ")
        model_path = CACHE_DIR / "models--meta-llama--Meta-Llama-3-8B-Instruct"
        if model_path.exists():
            print(f"   å­˜å‚¨è·¯å¾„: {model_path}")

        # é‡Šæ”¾å†…å­˜
        del model, tokenizer
        import gc
        gc.collect()
        return True

    except Exception as e:
        error_msg = str(e)
        if "401" in error_msg or "gated" in error_msg.lower():
            print("\nâŒ æƒé™é”™è¯¯: æ— æ³•è®¿é—®Llama-3æ¨¡å‹")
            print("\nè§£å†³æ–¹æ¡ˆ:")
            print("1. è®¿é—®: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct")
            print("2. ç‚¹å‡» 'Access repository' æ¥å—ä½¿ç”¨åè®®")
            print("3. è®¾ç½®HuggingFace token:")
            print("   a) è®¿é—®: https://huggingface.co/settings/tokens")
            print("   b) åˆ›å»ºæˆ–å¤åˆ¶token")
            print("   c) è¿è¡Œ: huggingface-cli login")
            print("   d) ç²˜è´´token")
            print("\næˆ–è€…ä½¿ç”¨æ›¿ä»£æ¨¡å‹ Mistral-7B-Instruct (è§ä¸‹æ–¹æç¤º)")
        else:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def download_mistral_alternative():
    """ä¸‹è½½Mistral-7B-Instructä½œä¸ºæ›¿ä»£æ–¹æ¡ˆ"""
    print("\n[æ›¿ä»£æ–¹æ¡ˆ] ä¸‹è½½ Mistral-7B-Instruct æ¨¡å‹...")
    print("-" * 60)
    print("æ¨¡å‹: mistralai/Mistral-7B-Instruct-v0.2")
    print("å¤§å°: ~14GB")
    print("è¯´æ˜: å¦‚æœæ— æ³•è·å–Llama-3è®¿é—®æƒé™ï¼Œå¯ä½¿ç”¨æ­¤æ¨¡å‹")

    response = input("\næ˜¯å¦ä¸‹è½½Mistral-7B-Instruct? (y/n): ").strip().lower()
    if response != 'y':
        print("è·³è¿‡Mistralä¸‹è½½")
        return False

    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer

        model_name = "mistralai/Mistral-7B-Instruct-v0.2"

        print("\nä¸‹è½½ tokenizer...")
        # ä¸æŒ‡å®šcache_dirï¼Œä½¿ç”¨HuggingFaceé»˜è®¤è·¯å¾„
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("âœ… Tokenizer ä¸‹è½½æˆåŠŸ")

        print("\nä¸‹è½½ æ¨¡å‹...")
        # ä¸æŒ‡å®šcache_dirï¼Œä½¿ç”¨HuggingFaceé»˜è®¤è·¯å¾„
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map=None,
            low_cpu_mem_usage=True
        )
        print("âœ… mistralai/Mistral-7B-Instruct-v0.2 ä¸‹è½½æˆåŠŸ")
        model_path = CACHE_DIR / "models--mistralai--Mistral-7B-Instruct-v0.2"
        if model_path.exists():
            print(f"   å­˜å‚¨è·¯å¾„: {model_path}")

        del model, tokenizer
        import gc
        gc.collect()
        return True

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def verify_downloads():
    """éªŒè¯æ‰€æœ‰æ¨¡å‹æ˜¯å¦ä¸‹è½½æˆåŠŸ"""
    print("\n" + "=" * 60)
    print("éªŒè¯ä¸‹è½½ç»“æœ")
    print("=" * 60)

    models_to_check = {
        "bert-base-uncased": "models--bert-base-uncased",
        "nomic-ai/nomic-embed-text-v1": "models--nomic-ai--nomic-embed-text-v1",
        "meta-llama/Meta-Llama-3-8B-Instruct": "models--meta-llama--Meta-Llama-3-8B-Instruct",
        "mistralai/Mistral-7B-Instruct-v0.2": "models--mistralai--Mistral-7B-Instruct-v0.2",
    }

    results = {}
    for model_name, cache_name in models_to_check.items():
        model_path = Path(CACHE_DIR) / cache_name
        if model_path.exists():
            # è®¡ç®—ç›®å½•å¤§å°
            total_size = sum(f.stat().st_size for f in model_path.rglob('*') if f.is_file())
            size_gb = total_size / (1024**3)
            print(f"âœ… {model_name}")
            print(f"   è·¯å¾„: {model_path}")
            print(f"   å¤§å°: {size_gb:.2f} GB")
            results[model_name] = True
        else:
            print(f"âŒ {model_name} - æœªæ‰¾åˆ°")
            results[model_name] = False

    return results

def main():
    """ä¸»å‡½æ•°"""
    print("\nå¼€å§‹ä¸‹è½½æµç¨‹...\n")

    # æ£€æŸ¥GPU
    has_gpu = check_gpu()

    # æ£€æŸ¥å¿…éœ€çš„PythonåŒ…
    print("\næ£€æŸ¥Pythonä¾èµ–...")
    try:
        import torch
        import transformers
        print(f"âœ… torch {torch.__version__}")
        print(f"âœ… transformers {transformers.__version__}")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·å…ˆå®‰è£…: pip install torch transformers")
        sys.exit(1)

    # ä¸‹è½½æ¨¡å‹
    success = []

    # 1. BERT tokenizer
    if download_bert_tokenizer():
        success.append("BERT")

    # 2. Nomic embedder
    if download_nomic_embedder():
        success.append("Nomic")

    # 3. Llama-3 inserter
    llama_success = download_llama3_inserter()
    if llama_success:
        success.append("Llama-3")
    else:
        # å¦‚æœLlama-3å¤±è´¥ï¼Œæä¾›Mistralæ›¿ä»£æ–¹æ¡ˆ
        if download_mistral_alternative():
            success.append("Mistral")

    # éªŒè¯ä¸‹è½½
    print("\n" + "=" * 60)
    results = verify_downloads()

    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ä¸‹è½½æ€»ç»“")
    print("=" * 60)
    print(f"æˆåŠŸä¸‹è½½: {len(success)} ä¸ªæ¨¡å‹")
    for model in success:
        print(f"  âœ… {model}")

    # æ£€æŸ¥æ˜¯å¦æœ‰inserteræ¨¡å‹
    has_inserter = results.get("meta-llama/Meta-Llama-3-8B-Instruct", False) or \
                   results.get("mistralai/Mistral-7B-Instruct-v0.2", False)

    if results.get("bert-base-uncased") and \
       results.get("nomic-ai/nomic-embed-text-v1") and \
       has_inserter:
        print("\nğŸ‰ æ‰€æœ‰å¿…éœ€æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        print(f"\næ¨¡å‹å­˜å‚¨ä½ç½®: {CACHE_DIR}")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. ä»Google Driveä¸‹è½½ paragram_xxl.pkl å’Œ nomic_embs.pkl")
        print("2. è¿è¡ŒPostMarkå°è£…ä»£ç ")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        if not has_inserter:
            print("\næ³¨æ„: æ°´å°æ’å…¥å™¨æ¨¡å‹æœªæˆåŠŸä¸‹è½½")
            print("      è¯·è§£å†³Llama-3è®¿é—®æƒé™é—®é¢˜æˆ–ä¸‹è½½Mistralæ›¿ä»£")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    # Manual download instructions
    print("\n" + "=" * 70)
    print("[2/2] Manual Download Required - PKL Files")
    print("=" * 70)
    print("\nPlease download the following files from Google Drive:\n")
    print("ğŸ“¦ Required files:")
    print("  1. paragram_xxl.pkl (~1-2GB)")
    print("  2. filtered_data_100k_unique_250w_sentbound_nomic_embs.pkl (~1GB)\n")
    print("ğŸ”— Google Drive link:")
    print("  https://drive.google.com/drive/folders/1Rdpqbtvy2s91ZrcgqDy6CrTCb9dZBQBf\n")
    print("ğŸ“ Place downloaded files in:")
    print(f"  {os.path.abspath('src/text_watermark/PostMark')}")
    print("\n" + "=" * 70)